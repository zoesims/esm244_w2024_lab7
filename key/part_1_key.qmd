---
title: 'ESM 244 Lab Week 7: Binomial logistic regression'
author: "Casey Oâ€™Hara"
date: "2024-02-15"
format: 
  html:
    embed-resources: true
    code-fold: true
    toc: true
execute:
  warning: false
  message: false
---

```{r setup}
library(tidyverse)
library(tidymodels)
library(palmerpenguins)
library(GGally)
library(jtools)
library(AICcmodavg)
```

# Intro

In lectures this week, we are learning about logistic regression - where based on predictor variables, we can estimate the probability of different discrete outcomes for a categorical variable. If there are only two mutually exclusive outcomes, we might use *binary logistic regression*, and for > 2 mutually exclusive outcomes we might use *multinomial logistic regression.* If the dependent variable is *ordinal* (discrete levels with meaningful order), we might use *ordinal logistic regression*.

Here, we will use *binary logistic regression* to find probabilities that a penguin is species Chinstrap or Adelie, based on several variables.  We'll compare the performance of two competing models using AIC, BIC, and cross validation, based on how accurately they classify the penguin species.

Pseudocode:

* Examine the data
* Identify the question
* Wrangle the data
* Identify candidate models
* Select among candidate models using AIC, BIC
* Select among candidate models using k-fold cross validation

# 1. Binary logistic regression

## a. Exploration with `ggpairs`

First, let's remind ourselves of the penguins data. We will only consider variables species, bill depth, bill length, body mass, flipper length and sex.

```{r}
penguins %>%
  select(species, bill_length_mm:sex) %>%
  GGally::ggpairs(aes(color = species))
```

We want to explore the relationship between bill length, depth, mass, flipper length, and sex (those will be our explanatory variables) and penguin species (that's our dependent variable).

To start, we'll just choose two species (those that are *most similar* across these variables to make it interesting), Adelie and Chinstrap penguins.

## b. Make subset with Adelie and Chinstrap penguins

```{r}
# note species is a factor
class(penguins$species)
levels(penguins$species)
 
adelie_chinstrap <- penguins %>%
  filter(species %in% c("Adelie", "Chinstrap")) %>%
  mutate(species = fct_drop(species)) %>%
  select(-year) %>%
  drop_na()
# This will drop a factor level that doesn't appear (otherwise Gentoo will 
# still show up as a factor level, even though there are no remaining 
# observations for Gentoo penguins...)
 
# Check the levels (note here Adelie is before Chinstrap, so Adelie 
# will be 0, Chinstrap will be 1)
class(adelie_chinstrap$species)
levels(adelie_chinstrap$species)
```

## c. Let's just check out trends across variables for those two species

```{r}
ggplot(data = adelie_chinstrap, aes(x = body_mass_g, y = flipper_length_mm)) +
  geom_point(aes(color = sex, shape = island)) +
  facet_wrap(~species)


ggplot(data = adelie_chinstrap, aes(x = body_mass_g, y = bill_length_mm)) +
  geom_point(aes(color = sex, shape = island)) +
  facet_wrap(~species)


```

## d. Binary logistic regression

Let's first try to predict penguin species as a function of body mass, flipper length, and sex

```{r}
f1 <- species ~ body_mass_g + flipper_length_mm + sex


ad_chin_blr1 <- glm(formula = f1,
                    data = adelie_chinstrap,
                    family = "binomial")
```

Look at the model:
```{r}
ad_chin_blr1
 
summary(ad_chin_blr1)
 
# Get a tidy version w/ broom:
blr1_tidy <- broom::tidy(ad_chin_blr1)
```

How can we start thinking about this?

- These are coefficients for the log-linear model (e.g. these are coefficients for the predictor variables that relate to the *log odds* of the "Chinstrap" outcome).

- The null hypothesis for coefficients is that they = 0

- The coefficient for body mass, `r round(blr1_tidy[2,2], 5)`, indicates that on average we expect the log odds of a penguin being a Chinstrap (remember, that's the '1' outcome) decreases by `r round(blr1_tidy[2,2], 5)` for each 1 g increase in penguin body mass (see `blr1_tidy` - this coefficient is not significant).
 
Does this align with the mass comparisons for Chinstraps & Adelies we see?
```{r}
ggplot(data = adelie_chinstrap, aes(x = species, y = body_mass_g)) +
  geom_jitter(aes(color = sex))
```

- The coefficient for flipper length, `r round(blr1_tidy[3,2], 2)`, indicates that on average we expect the log odds of a penguin being a Chinstrap (remember, that's the '1' outcome) increases by `r round(blr1_tidy[3,2], 2)` for each 1 mm increase in penguin flipper length (see `blr1_tidy` - this coefficient is significant).

Does this align with the flipper comparisons for Chinstraps & Adelies we see?

```{r}
ggplot(data = adelie_chinstrap, aes(x = species, y = flipper_length_mm)) +
  geom_jitter(aes(color = sex))
```
- The coefficient for sex, `r round(blr1_tidy[4,2], 2)`, indicates that on average we expect the log odds of a penguin being a Chinstrap (remember, that's the '1' outcome) decreases by `r round(blr1_tidy[4,2], 2)` if the penguin is Male, compared to Female 
    - this is a weird example -- but perhaps one species has a different natural balance between males and females?  
    - you can imagine relevant interpretations for other scenarios e.g. "The odds of supporting a bill for conservation (Y/N) increases if the individual identifies as an Environmentalist, compared to those who identify as Not an Environmentalist."

But log odds are challenging to interpret. Let's find actual *probabilities* associated with a penguin being Adelie or Chinstrap, based on the selected variables and the model outcome.

Adding `type.predict = "response"` here converts the log odds (link), the default reported, to the probability of being Chinstrap for each observation.

```{r}
blr1_fitted <- ad_chin_blr1 %>%
  broom::augment(type.predict = "response")
```

Look at the outcome data frame.

That shows us the probability (in the `.fitted` column) of a penguin being a Chinstrap based on the three variables `body_mass_g`, `flipper_length_mm`, and `sex`. Take a moment to look through the probabilities. Are there some that have a high probability of being a Chinstrap, but are actually Adelies? YES (e.g. Row 91 shows a probability of 0.78 of being a Chinstrap, based on this model...). But *most* of the actual Adelies in the dataset have a higher probability of being an Adelie based on the model (probability of a Chinstrap < 0.5).

A number of the actual Chinstraps (if we weren't looking at the actual observation) have, based on the model, a higher probability of being an Adelie by classification. This demonstrates why, in Machine Learning, we need a training dataset (which we'd use to create the model), then a totally separate test dataset to see how successfully it classifies the outcome (e.g. penguin species here).

Let's do a couple of quick visualizations, with flipper length (the only significant coefficient) on the x-axis and probability of being a Chinstrap on the y-axis:
```{r}
ggplot(data = blr1_fitted, aes(x = flipper_length_mm, y = .fitted)) +
  # add aes(shape = species) to compare probability with actual
  geom_point(aes(color = sex, shape = species)) +
  # add geom_smooth to show general fit
  geom_smooth(aes(color = sex), se = FALSE) +
  labs(x = "Flipper length (mm)",
   	   y = "Probability of outcome Chinstrap")
```

## Visualization of p(Chinstrap) by variable

The `jtools::effect_plot()` function provides some quick model plotting. Note: for more customized visualization of model predictions, you may want to create a new "test" data frame of theoretical values, then use the `predict()` function to append predicted probabilities before plotting in `ggplot()`.

```{r}
# For flipper length:
jtools::effect_plot(ad_chin_blr1,
        	pred = flipper_length_mm,
        	interval = TRUE,
        	y.label = "Probability of 'Chinstrap'")
 
# For body mass:
effect_plot(ad_chin_blr1,
        	pred = body_mass_g,
        	interval = TRUE,
          	y.label = "Probability of 'Chinstrap'")
```

## Predictions for new values with `predict()`

What is the probability that a female penguin weight 3410 g with a flipper length of 192 mm will be Chinstrap?

```{r}
ex_1 <- predict(ad_chin_blr1,
                data.frame(sex = "female",
                  body_mass_g = 3410,
                  flipper_length_mm = 192),
                # tell it type = 'response' to get prob, not log odds
                type = "response")
 
# Based on the model, the probability that this penguin is a Chinstrap is 0.4.
```

You can also feed in a new data frame, with multiple penguin observations, to get model probability estimates for more than one penguin:

```{r}
new_df <- data.frame(
  sex = c("male", "male", "female"),
  body_mass_g = c(3298, 4100, 3600),
  flipper_length_mm = c(212, 175, 180)
)
 
ex_2 <- predict(ad_chin_blr1,
            	    new_df,
            	    type = "response")
```


## e. Binary logistic regression - new model

From the ggpairs plot, we saw that bill length might be a good predictor.  Let's now try to predict penguin species as a function of just bill length...

```{r}
f2 <- species ~ bill_length_mm + body_mass_g


ad_chin_blr2 <- glm(formula = f2,
                    data = adelie_chinstrap,
                    family = "binomial")
```

Look at the model:
```{r}
ad_chin_blr2
 
summary(ad_chin_blr2)
 
# Get a tidy version w/ broom:
blr2_tidy <- broom::tidy(ad_chin_blr2)
```


Let's see if this makes sense based on a visual comparison:
```{r}
ggplot(adelie_chinstrap, aes(x = bill_length_mm, y = body_mass_g)) +
  geom_point(aes(color = species))
```

Let's visualize the results for this model like we did before:
``` {r}
effect_plot(ad_chin_blr2,
        	pred = bill_length_mm,
        	interval = TRUE,
        	y.label = "Probability of 'Chinstrap'")


effect_plot(ad_chin_blr2,
        	pred = body_mass_g,
        	interval = TRUE,
        	y.label = "Probability of 'Chinstrap'")


```

## Model selection

Let's compare the models using AICc and BIC
```{r}
AICcmodavg::aictab(list(ad_chin_blr1, ad_chin_blr2))
AICcmodavg::bictab(list(ad_chin_blr1, ad_chin_blr2))
```

And let's compare with a 10-fold cross-validation, using prediction accuracy as our metric.

``` {r}
set.seed(123)


n_folds <- 10
fold_vec <- rep(1:n_folds, length.out = nrow(adelie_chinstrap))
ad_chin_kfold <- adelie_chinstrap %>%
  mutate(fold = sample(fold_vec, size = n(), replace = FALSE))

```

# for-loop version (SKIP FOR LAB - include as reference)

```{r}
results_df <- data.frame()
pred_acc <- function(x, y) {
  accurate <- ifelse(x == y, 1, 0)
  return(mean(accurate, na.rm = TRUE))
}

for(i in 1:n_folds) {
  kfold_test <- ad_chin_kfold %>%
    filter(fold == i)
  kfold_train <- ad_chin_kfold %>%
    filter(fold != i)
  
  kfold_blr1 <- glm(f1, data = kfold_train, family = 'binomial')
  kfold_blr2 <- glm(f2, data = kfold_train, family = 'binomial')
  kfold_pred <- kfold_test %>%
    mutate(blr1 = predict(kfold_blr1, kfold_test, type = 'response'),
           blr2 = predict(kfold_blr2, ., type = 'response')) %>%
    mutate(pred1 = ifelse(blr1 > 0.50, 'Chinstrap', 'Adelie'),
           pred2 = ifelse(blr2 > 0.50, 'Chinstrap', 'Adelie'))
  kfold_accuracy <- kfold_pred %>%
    summarize(blr1_acc = pred_acc(species, pred1),
              blr2_acc = pred_acc(species, pred2))
  
  results_df <- bind_rows(results_df, kfold_accuracy)
}


results_df %>%
  summarize(blr1_acc = mean(blr1_acc),
            blr2_acc = mean(blr2_acc))
```

# purrr::map version: returns a list

```{r}
x_vec <- 1:10

thing <- purrr::map(.x = x_vec, # a sequence (vector, list)
                    .f = sqrt)  # name of a function (without parens)

my_funct <- function(x, y, z) {
  return((x - y) ^ z)
}

thing2 <- purrr::map(.x = x_vec,      # a sequence (for first arg of function)
                     .f = my_funct,   # name of a function to apply
                     y = 2, z = 3)    # additional parameters (for other args)
```

``` {r}
# function to calculate accuracy, given a "truth" vector and "prediction" vector
pred_acc <- function(x, y) {
  accurate <- ifelse(x == y, 1, 0)
  
  return(mean(accurate, na.rm = TRUE))
}

# function to calculate accuracy of BLR of one fold (training and testing)
calc_fold <- function(i, fold_df, f) {
  kfold_test <- fold_df %>%
    filter(fold == i)
  kfold_train <- fold_df %>%
    filter(fold != i)
  
  kfold_blr <- glm(f, data = kfold_train, family = 'binomial')
  kfold_pred <- kfold_test %>%
    mutate(blr = predict(kfold_blr, kfold_test, type = 'response')) %>%
    mutate(pred = ifelse(blr > 0.50, 'Chinstrap', 'Adelie'))
  
  kfold_accuracy <- kfold_pred %>%
    summarize(blr_acc = pred_acc(species, pred)) # using my other function
  
  return(kfold_accuracy)
}

n_folds <- 10

results1_purrr_df <- purrr::map(.x = 1:n_folds, # sequence of fold numbers
                                .f = calc_fold, # function
                                fold_df = ad_chin_kfold, # additional argument to calc_fold()
                                f = f1) %>%              # additional argument to calc_fold()
  bind_rows() %>%
  mutate(mdl = 'f1')

results2_purrr_df <- purrr::map(.x = 1:n_folds, .f = calc_fold, 
                               fold_df = ad_chin_kfold,
                               f = f2) %>%
  bind_rows() %>%
  mutate(mdl = 'f2')

results_purrr_df <- bind_rows(results1_purrr_df, results2_purrr_df) %>%
  group_by(mdl) %>%
  summarize(mean_acc = mean(blr_acc))

results_purrr_df
```

Which model seems best?  Does this agree with AIC and BIC selection?


# Tidymodels flow

See https://www.tidymodels.org/ for tons of details and tutorials!  Tidymodels (and parsnip) packages clean up and standardize the output from hundreds of different modeling functions from dozens of different modeling packages.  For example, binomial logistic regression algorithms show up in quite a few different modeling packages, but the arguments and outputs differ from package to package - annoying!

Not going to get into: "recipes" for pre-processing, "workflows" 

## Tidymodels basic

```{r}
### Set the model type
?logistic_reg ### note glm is the default engine

blr_model <- logistic_reg() %>% ### also linear_reg, rand_forest, etc
  set_engine('glm')

### basic regression
blr_tidyfit_f1 <- blr_model %>%
  fit(f1, data = adelie_chinstrap)
blr_tidyfit_f2 <- blr_model %>%
  fit(f2, data = adelie_chinstrap)

### query the fitted models
blr_tidyfit_f1
blr_tidyfit_f2

### examine different outputs to see how well the models fit
blr_tidyfit_f1 %>%
  tidy()

blr_tidyfit_f1 %>%
  glance()

```

## Tidymodels crossfold validation

```{r}
### set seed for reproducibility! here to set the folds
set.seed(345)

tidy_folds <- vfold_cv(adelie_chinstrap, v = 10)
tidy_folds

### use a workflow that bundles the logistic model and a formula
# blr_model <- logistic_reg() %>%
#   set_engine('glm')

blr_tidy_wf1 <- workflow() %>%
  add_model(blr_model) %>%
  add_formula(f1)

blr_tidy_cv_f1 <- blr_tidy_wf1 %>%
  fit_resamples(tidy_folds)

### use functions from the tune package to extract metrics
collect_metrics(blr_tidy_cv_f1)
#   .metric  .estimator  mean     n std_err .config             
#   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               
# 1 accuracy binary     0.828    10 0.00739 Preprocessor1_Model1
# 2 roc_auc  binary     0.902    10 0.00808 Preprocessor1_Model1

### We'll talk about roc_auc next week!


### Repeat for model 2 - let students do this on their own!
blr_tidy_wf2 <- workflow() %>%
  add_model(blr_model) %>%
  add_formula(f2)

blr_tidy_cv_f2 <- blr_tidy_wf2 %>%
  fit_resamples(tidy_folds)

### use functions from the tune package to extract metrics
collect_metrics(blr_tidy_cv_f2)

```

## Area under the curve!

Receiver Operating Characteristic Curve (ROC Curve) compares the diagnostic ability of a binary classifier (like logistic regression) based on the discrimination threshold.  Up to now (and for homework) we've been using a 50% threshold by default.  The ROC can tell us tradeoffs between true positive rate and false positive rate as we change the threshold, and also can give a great indication of model quality.

It seems like model 2 is far better than model 1 in this instance.

```{r}
### This is copied from above, for reference
# blr_model <- logistic_reg() %>% ### also linear_reg, rand_forest, etc
#   set_engine('glm')
# 
# ### basic regression
# blr_tidyfit_f1 <- blr_model %>%
#   fit(f1, data = adelie_chinstrap)
# blr_tidyfit_f2 <- blr_model %>%
#   fit(f2, data = adelie_chinstrap)

blr_f1_pred <- adelie_chinstrap %>%
  mutate(predict(blr_tidyfit_f1, .),
         predict(blr_tidyfit_f1, ., type = 'prob'))

blr_f1_pred %>%
  roc_curve(truth = species, .pred_Adelie) %>%
  autoplot()

blr_f1_pred %>%
  roc_auc(truth = species, .pred_Adelie)

### Students repeat for blr_tidyfit_f2 and compare!
blr_f2_pred <- adelie_chinstrap %>%
  mutate(predict(blr_tidyfit_f2, .),
         predict(blr_tidyfit_f2, ., type = 'prob'))

blr_f2_pred %>%
  roc_curve(truth = species, .pred_Adelie) %>%
  autoplot()

blr_f2_pred %>%
  roc_auc(truth = species, .pred_Adelie)

```

# End Part 1
